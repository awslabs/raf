<!--- Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved. -->
<!--- SPDX-License-Identifier: Apache-2.0  -->
<!--- Auto generated by docs/wiki/gen_readme.py. Do not touch. -->
## Motivation

**Meta framework.** Because, first, it is not a single framework, but meta by nature. With TVM Relay frontend importers, we are able to absort any TensorFlow model, ONNX model, into our core intermediate representation, and mix it with each other - as a researcher, just imagine that you are able to define the tedious part of your network in Keras, like backbones in vision, and we allow you to focus on manipulating the awesome part of your neural network using our numpy-like API.

**Compiler native.** Second, it is said that it is boring to develop another one because all frontends are converging. True. While we converge to Numpy + Gluon/Keras, we are the first compiler-native framework, which is the key feature that makes us the most outstanding. We are the missing frontend of TVM/Relay stack for intelligent compiler. Once you implement your model using our framework, our JIT is in charge of fusing operators, finding the best data layout for training.

**Python-Relay Transpiler.** Third, we are capable of translating Python to the properly-designed intermediate representation, Relay, supporting arbitrary control flow (nested loop/if/continue/break/return), mutual function calls and wide range of types, including primitive types\* and container types (tuple, list, dict), and classes\*\* as well as back propogation through them. In the meantime, we are able to hook in almost all developer/user-defined functions to enrich our system via either true hybridization or packed function registration.

\* Integers, floating point numbers, bool, and potentially strings.

\*\* Technically, we support class through named tuple, which is slightly different.





## Table of Contents
- Getting Start
    - [Build On Conda](1_getting_start/Build-on-Conda.md)
    - [Build On MacOS](1_getting_start/Build-on-macOS.md)
    - [Build On Ubuntu 18.04](1_getting_start/Build-on-Ubuntu-18.04.md)
- User Guide
    - [AMP](2_user_guide/AMP.md)
    - [Distributed Training](2_user_guide/Distributed-Training.md)
    - [Train Model](2_user_guide/Train-Model.md)
    - [Train PyTorch Model](2_user_guide/Train-PyTorch-Model.md)
- Dev Guide
    - [Add Operator](3_dev_guide/Add-Operator.md)
    - [Add Pass](3_dev_guide/Add-Pass.md)
    - [Analyze Model](3_dev_guide/Analyze-Model.md)
    - [Distribution Mechanism](3_dev_guide/Distribution-Mechanism.md)
    - [IR Format](3_dev_guide/IR-Format.md)
    - [Memory Pool](3_dev_guide/Memory-Pool.md)
    - [Op Dialect](3_dev_guide/Op-Dialect.md)
    - Pass
        - [AutoDataParallel](3_dev_guide/pass/AutoDataParallel.md)
        - [Multi CUDA Stream Schedule](3_dev_guide/pass/Multi-CUDA-Stream-Schedule.md)
        - [Rematerialization](3_dev_guide/pass/Rematerialization.md)
    - [Profile Model](3_dev_guide/Profile-Model.md)
- Contrib Guide
    - [Development And Pull Request](4_contrib_guide/Development-And-Pull-Request.md)
    - [RFC Collections](4_contrib_guide/RFC-Collections.md)
